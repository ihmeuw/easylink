.. _getting_started:

===============
Getting Started
===============

Introduction
============
EasyLink is a framework that allows users to build and run highly configurable record linkage pipelines, 
as demonstrated in this tutorial. EasyLink allows users to "mix and match" different pieces of record 
linkage software, by ensuring that each piece of the pipeline conforms to standard patterns. 

For example, users at the Census Bureau could easily evaluate whether using a more sophisticated "blocking" 
method would improve results in a certain pipeline, without having to rewrite the entire pipeline. For more 
information, read about the `motivation <https://easylink.readthedocs.io/en/latest/concepts/pipeline_schema/index.html#motivation>`_
behind EasyLink.

Overview
--------
This tutorial introduces EasyLink concepts and features by demonstrating the software's usage. Covered 
concepts include the EasyLink record linkage pipeline schema, Easylink pipeline configuration, running 
pipelines, changing record linkage step implementations, changing input data, evaluating and comparing 
results, and more. 

.. contents::

Audience
--------
This tutorial is intended for people familiar with record linkage practices, who are interested
in easily comparing linkage results across different methods. This tutorial will *not* cover information 
about linkage techniques or software and assumes the reader is familiar with these concepts and 
ready to implement that knowledge using EasyLink.

Tutorial prerequisites
----------------------
`Install EasyLink <https://github.com/ihmeuw/easylink>`_ if you haven't already. 

The tutorial uses the `Splink <https://moj-analytical-services.github.io/splink/index.html>`_ Python package 
for record linkage implementations. Splink knowledge is not required to complete the tutorial but may be 
helpful when configuring Splink models.


Configuring a "naive" Splink model
==================================
Our first demonstration of running an EasyLink pipeline will configure a simple, "naive" record linkage
model using Splink implementations. Our pipeline will link the 
`pseudopeople <https://pseudopeople.readthedocs.io/en/latest/>`_ 
``Social Security Administration`` and ``Tax forms: W-2 & 1099`` 
`datasets <https://pseudopeople.readthedocs.io/en/latest/datasets/index.html>`_.

``easylink run`` parameters
---------------------------
The command we will use to run the pipeline will look like this:

.. code-block:: console

   $ easylink run -p pipeline_demo_naive.yaml -i input_data_demo.yaml -e environment_local.yaml -I /mnt/team/simulation_science/priv/engineering/er_ecosystem/images

.. todo:: 
    Remove ``-I`` flag (or change and add ``-I`` section) so that images will be downloaded from Zenodo when that is ready

More information is available in the :ref:`easylink run <cli>` command docs, but let's briefly review each 
parameter and file we will pass to the command.

Input data
^^^^^^^^^^
The ``--input-data`` (``-i``) argument to ``easylink run`` accepts a YAML file specifying a list 
of paths to files or directories containing input data to be used by the pipeline. 
The contents of 
:download:`input_data_demo.yaml <input_data_demo.yaml>` are shown below -- download it to the 
directory from which you will run ``easylink run``. 

.. code-block:: yaml

    input_file_ssa_2020: /mnt/team/simulation_science/priv/engineering/er_ecosystem/input_data/tylerdy/input_file_ssa_2020.parquet
    input_file_w2_2020: /mnt/team/simulation_science/priv/engineering/er_ecosystem/input_data/tylerdy/input_file_w2_2020.parquet
    known_clusters: /mnt/team/simulation_science/priv/engineering/er_ecosystem/input_data/main/known_clusters.parquet

Here we have defined the locations of the three input files we will use: the 2020 versions of the 
``Social Security Administration`` and ``W2 & 1099`` datasets, and an empty ``known_clusters`` file, since no
clusters are known to us before running this pipeline. 

.. note::
    To meet the input specifications for :ref:`datasets` defined by the pipeline schema,
    the ``SSA`` and ``W2`` datasets, after being generated by pseudopeople, were modified
    to add the required ``Record ID`` column. ``SSA`` death records were also removed, 
    leaving only ``creation`` type records.
  

Pipeline specification
^^^^^^^^^^^^^^^^^^^^^^
The ``--pipeline-specification`` (``-p``) argument to ``easylink run`` accepts a YAML file specifying 
the implementations and other configuration options for the pipeline being run. The contents of 
:download:`pipeline_demo_naive.yaml <pipeline_demo_naive.yaml>` are shown below -- download it to the 
directory from which you will run ``easylink run``. 

The pipeline specification follows the structure defined in the :ref:`pipeline_schema`. This document
is the part of EasyLink that enforces the standard patterns that linkage step implementations must 
follow, enabling easy configuration and swapping. Examples of patterns defined in the document include 
breaking the record linkage process into a set of steps by which most common record linkage implementations
can be described, providing a set of operators allowing users to customize how data flows through those
steps, and enforcing specifications for the format of step inputs and outputs. The set of steps, their 
hierarchy, and an operator example (``parallel``) can be seen below.

.. code-block:: yaml

  steps:
    entity_resolution:
      substeps:
        determining_exclusions_and_removing_records:
          parallel:
            - determining_exclusions:
                implementation:
                  name: default_determining_exclusions
                  configuration:
                    INPUT_DATASET: input_file_ssa_2020
              removing_records:
                implementation:
                  name: default_removing_records
                  configuration:
                    INPUT_DATASET: input_file_ssa_2020
            - determining_exclusions:
                implementation:
                  name: default_determining_exclusions
                  configuration:
                    INPUT_DATASET: input_file_w2_2020
              removing_records:
                implementation:
                  name: default_removing_records
                  configuration:
                    INPUT_DATASET: input_file_w2_2020
        clustering:
          substeps:
            clusters_to_links:
              implementation:
                name: default_clusters_to_links
            linking:
              substeps:
                pre-processing:
                  parallel:
                  - implementation:
                      name: dummy_pre-processing
                      configuration: 
                        INPUT_DATASET: input_file_ssa_2020
                  - implementation:
                      name: dummy_pre-processing
                      configuration: 
                        INPUT_DATASET: input_file_w2_2020
                schema_alignment:
                  implementation:
                    name: default_schema_alignment
                blocking_and_filtering:
                  implementation:
                    name: splink_blocking_and_filtering
                    configuration:
                      BLOCKING_RULES: "'l.first_name == r.first_name,l.last_name == r.last_name'"
                      LINK_ONLY: true
                evaluating_pairs:
                  implementation:
                    name: splink_evaluating_pairs
                    configuration:
                      BLOCKING_RULES_FOR_TRAINING: "'l.first_name == r.first_name,l.last_name == r.last_name'"
                      COMPARISONS: "'ssn:exact,first_name:exact,middle_initial:exact,last_name:exact'"
                      PROBABILITY_TWO_RANDOM_RECORDS_MATCH: 0.01 
                      THRESHOLD_MATCH_PROBABILITY: 0
                      LINK_ONLY: true
            links_to_clusters:
              implementation:
                name: splink_links_to_clusters
                configuration:
                  THRESHOLD_MATCH_PROBABILITY: 0.997
        updating_clusters:
          implementation:
            name: default_updating_clusters
    canonicalizing_and_downstream_analysis:
      implementation:
        name: dummy_canonicalizing_and_downstream_analysis

Structure
"""""""""

Let's break down the configuration keys and values defined in the file. 
First, note that all of the keys defined as direct children of a ``steps`` 
or ``substeps`` key represent record linkage steps from the 
:ref:`pipeline_schema`. They are nested in the same structure defined in 
that document. For example, :ref:`linking_sub_steps` and the ``linking`` YAML 
key both list the same substeps -- ``pre-processing``, 
``schema_alignment``, ``blocking_and_filtering``, and ``evaluating_pairs``.

Now that we understand the nested step structure of the pipeline specification 
YAML, let's discuss the keys used to configure individual steps.

.. _implementation_configuration:

Implementation configuration
""""""""""""""""""""""""""""
We can see in the YAML that many steps use all three of the ``implementation``, ``name`` 
and ``configuration`` keys, as well as implementation-specific keys. Let's look at 
``links_to_clusters`` as an example.

The ``implementation`` section simply indicates that the subkeys that follow define the
step's implementation in this pipeline.

The ``name`` key selects which of the available implementations for this step will 
be used.

.. todo:: 
    Link to docs for "available implementations" for each step when that is available.

The ``configuration`` section lists implementation-specific configuration keys
which control how the implementation will run. For example, ``THRESHOLD_MATCH_PROBABILITY`` 
here allows the user to define at what probability a pair of records being considered 
as a pontential link will be considered part of the same cluster by the 
``splink_links_to_clusters`` implementation.

Cloneable Sections
""""""""""""""""""
Certain sections of the pipeline are defined as as :ref:`cloneable_sections`, which create 
multiple copies of that section and allow different implementations or inputs to be defined 
for each copy. We can see that :ref:`entity_resolution_sub_steps` defines
``determining_exclusions`` and ``removing_records`` as cloneable in the diagram 
(blue dashed box).

In the YAML, the superstep ``determining_exclusions_and_removing_records`` is marked as 
clonable using the ``parallel`` key, and two copies are made of its substeps, 
``determining_exclusions`` and ``removing_records``. The ``-`` denotes the beginning
of each of the two copies, each of which must contain both of the substeps. 

We can see that the only difference between the two copies is what filename is passed 
to the ``INPUT_DATASET`` environment variables for each step. In 
the first copy, the ``ssa`` dataset files are used as inputs for both steps, 
while in the second copy, the ``w2`` dataset files are the inputs. In practice, 
this means that records to exclude will be identified and removed separately for 
each input file, as required by the schema since each input file has different data. 
This cloneable section also allows different implementations to be used for each dataset 
if desired.

.. todo:: 
    Update cloneable keyword when we finalize it.

Computing Environment
"""""""""""""""""""""
The ``--computing-environment`` (``-e``) argument to ``easylink run`` accepts a YAML file specifying 
information about the computing environment which will execute the steps of the 
pipeline. The contents of :download:`environment_local.yaml <../../../../tests/specifications/common/environment_local.yaml>`
are shown below -- download it to the 
directory from which you will run ``easylink run``. 

.. code-block:: yaml

   computing_environment: local
   container_engine: singularity

It specifies a ``local`` computing environment using ``singularity`` as the container engine. These parameters indicate that no new compute resources will 
be used to execute the pipeline steps, and that the Singularity container for each implementation will run within the context where ``easylink run`` is being executed.
For example, if you ran the ``easylink run`` command on your laptop, the implementations would run on your laptop;
if you ran the ``easylink run`` command on a cloud (e.g. EC2) instance that you were connected to with SSH, the implementations would run on that instance,
and so on.

Configuring Splink
------------------
Having explained how the inputs, general pipeline format, and computing environment
are specified, now we will discuss how the pipeline specification configures 
our actual Splink record linkage model.

There are three Splink implementations in the pipeline specification YAML 
for us to configure: ``splink_blocking_and_filtering``, ``splink_evaluating_pairs``,
and ``splink_links_to_clusters``. Each of these implementations has its own variables 
to configure. The implementation ``middle_name_to_initial`` is used for the 
``pre_processing`` step for ``SSA`` data to create a column that maches the ``W2`` 
``middle_initial`` column.

For all other pipeline steps, we've selected a default implementation, which 
either does nothing or simply passes inputs to outputs as appropriate.


For ``splink_blocking_and_filtering``, we set::

    BLOCKING_RULES: "'l.first_name == r.first_name,l.last_name == r.last_name'"
    LINK_ONLY: true

These variables are used by the Splink implementation to define which pairs of records 
will be considered as possible matches (records with matching first or last names), 
and to instruct Splink to link records without first de-depulicating, respectively. 

For ``splink_evaluating_pairs``, we set::

    BLOCKING_RULES_FOR_TRAINING: "'l.first_name == r.first_name,l.last_name == r.last_name'"
    COMPARISONS: "'ssn:exact,first_name:exact,middle_initial:exact,last_name:exact'"
    PROBABILITY_TWO_RANDOM_RECORDS_MATCH: 0.01
    THRESHOLD_MATCH_PROBABILITY: 0
    LINK_ONLY: true  

The first variable is similar to what was set for the previous implementation. The second 
defines the columns which will be compared by the Splink model, and how Splink will evaluate
whether the column values match (exact comparisons). The third is a parameter used in training
the model. The fourth determines at what match probability a pair of records will be outputted
from the step (``0`` outputs all pairs). The fifth is used in the same way as in the previous
implementation.

For ``splink_links_to_clusters``, as discussed earlier in the :ref:`implementation_configuration` section,
we set::

    THRESHOLD_MATCH_PROBABILITY: 0.997    

Running the pipeline
====================
Now that we understand all the inputs to ``easylink run``, lets actually run the pipeline::

    $ easylink run -p pipeline_demo_naive.yaml -i input_data_demo.yaml -e environment_local.yaml -I /mnt/team/simulation_science/priv/engineering/er_ecosystem/images
    2025-06-17 10:40:24.859 | 0:00:02.515481 | run:196 - Running pipeline
    2025-06-17 10:40:24.860 | 0:00:02.516496 | run:198 - Results directory: /mnt/share/homes/tylerdy/easylink/docs/source/user_guide/tutorials/results/2025_06_17_10_40_24
    2025-06-17 10:40:51.886 | 0:00:29.542638 | main:124 - Running Snakemake
    [Tue Jun 17 10:40:52 2025]
    Job 19: Validating determining_exclusions_and_removing_records_parallel_split_2_determining_exclusions_default_determining_exclusions input slot known_clusters
    Reason: Missing output files: input_validations/determining_exclusions_and_removing_records_parallel_split_2_determining_exclusions_default_determining_exclusions/known_clusters_validator
    ...
    [Tue Jun 17 10:40:56 2025]
    Job 11: Running determining_exclusions implementation: default_determining_exclusions
    Reason: Missing output files: intermediate/determining_exclusions_and_removing_records_parallel_split_1_determining_exclusions_default_determining_exclusions/result.parquet; Input files updated by another job: input_validations/determining_exclusions_and_removing_records_parallel_split_1_determining_exclusions_default_determining_exclusions/input_datasets_validator, input_validations/determining_exclusions_and_removing_records_parallel_split_1_determining_exclusions_default_determining_exclusions/known_clusters_validator
    ...
    [Tue Jun 17 10:41:30 2025]
    Job 4: Running evaluating_pairs implementation: splink_evaluating_pairs
    Reason: Missing output files: intermediate/splink_evaluating_pairs/result.parquet; Input files updated by another job: input_validations/splink_evaluating_pairs/blocks_validator, intermediate/default_clusters_to_links/result.parquet, intermediate/splink_blocking_and_filtering/blocks, input_validations/splink_evaluating_pairs/known_links_validator
    ...
    [Tue Jun 17 10:42:09 2025]
    Job 0: Grabbing final output
    Reason: Missing output files: result.parquet; Input files updated by another job: intermediate/dummy_canonicalizing_and_downstream_analysis/result.parquet, input_validations/final_validator


.. note:: 
   The pipeline output in its current state can be a little confusing. Note that the number assigned 
   to the slurm jobs is different than the order the jobs are executed in - these job IDs are 
   assigned by snakemake. Also note that several input validation jobs will run before any actual 
   step implementations.

   Finally, despite the final output line containing the phrase "Missing output files", 
   this pipeline finished executing successfully. The "Reason" displayed in the output is explaining 
   why the job was run (the step inputs were ready but the output file did not yet exist), rather than 
   conveying an error message.

Inputs and outputs
------------------
Input and output data is stored in Parquet files. We can view the contents of the files listed in 
``input_data_demo.yaml`` using Python:

.. code-block:: console

   $ # Create/activate a conda environment if you don't want to install globally!
   $ pip install pandas pyarrow
   $ python
   >>> import pandas as pd
   >>> pd.read_parquet("/mnt/team/simulation_science/priv/engineering/er_ecosystem/input_data/tylerdy/input_file_ssa_2020.parquet")
        simulant_id          ssn first_name    middle_name       last_name date_of_birth     sex event_type event_date  Record ID middle_initial
    0         0_19979  786-77-6454     Evelyn  Granddaughter         Sorrell      19191204  Female   creation   19191204          0              G
    1          0_6846  688-88-6377     George         Robert           Kelly      19210616    Male   creation   19210616          1              R
    2         0_19983  651-33-9561   Beatrice         Jennie      Livingston      19220113  Female   creation   19220113          2              J
    3           0_262  665-25-7858       Eura         Nadine       Hutchison      19220305  Female   creation   19220305          3              N
    4         0_12473  875-10-2359    Roberta           Ruth        Mcintyre      19220306  Female   creation   19220306          4              R
    ...           ...          ...        ...            ...             ...           ...     ...        ...        ...        ...            ...
    16492     0_20687  183-90-0619    Matthew        Michael        Stephens      19800224  Female   creation   20201229      16492              M
    16493     0_20686  803-81-8527     Jermey          Tyler          Morris      19860415    Male   creation   20201229      16493              T
    16494     0_20692  170-62-5253  Brittanie         Lauren             Kim      19950118  Female   creation   20201229      16494              L
    16495     0_20662  281-88-9330     Marcus         Jasper            None      20201230    Male   creation   20201230      16495              J
    16496     0_20673  547-99-7034     Analia        Brielle  Ascencio Maria      20201231  Female   creation   20201231      16496              B
    [15984 rows x 11 columns]
    >>> pd.read_parquet("/mnt/team/simulation_science/priv/engineering/er_ecosystem/input_data/tylerdy/input_file_w2_2020.parquet")
        simulant_id household_id employer_id          ssn  wages  ... mailing_address_state mailing_address_zipcode tax_form tax_year Record ID
    0            0_4          0_8          95  584-16-0130  10192  ...                    WA                   00000       W2     2020         0
    1            0_5          0_8          29  854-13-6295  28355  ...                    WA                   00000       W2     2020         1
    2            0_5          0_8          30  854-13-6295  18243  ...                    WA                   00000       W2     2020         2
    3         0_5621       0_2289          46  674-27-1745   7704  ...                    WA                   00000       W2     2020         3
    4         0_5623       0_2289          83  794-23-1522   3490  ...                    WA                   00000       W2     2020         4
    ...          ...          ...         ...          ...    ...  ...                   ...                     ...      ...      ...       ...
    9898     0_18936       0_7621          23  006-92-7857   9585  ...                    WA                   00000       W2     2020      9898
    9899     0_18936       0_7621          90  006-92-7857  57906  ...                    WA                   00000       W2     2020      9899
    9900     0_18937       0_7621           1  182-82-5017  19609  ...                    WA                   00000     1099     2020      9900
    9901     0_18937       0_7621         105  182-82-5017   8061  ...                    WA                   00000     1099     2020      9901
    9902     0_18939       0_7621           9  283-97-5940   4961  ...                    WA                   00000       W2     2020      9902
    [9903 rows x 25 columns]

Recall that the ``known_clusters.parquet`` input file is empty.

It can also be useful to setup an alias to more easily preview parquet files. Add the following to your 
``.bash_aliases`` or ``.bashrc`` file, and restart your terminal.

.. code-block:: console

   pqprint() { python -c "import pandas as pd; print(pd.read_parquet('$1'))" ; }

Let's use the alias to print the results parquet, the location of which was printed when we ran the pipeline.

.. code-block:: console

   $ pqprint results/2025_06_17_10_40_24/result.parquet 
          Input Record Dataset  Input Record ID                    Cluster ID
    0      input_file_ssa_2020             7345   input_file_ssa_2020-__-7345
    1      input_file_ssa_2020             7346   input_file_ssa_2020-__-7346
    2      input_file_ssa_2020             7347   input_file_ssa_2020-__-7347
    3      input_file_ssa_2020             7348   input_file_ssa_2020-__-7348
    4      input_file_ssa_2020             7349   input_file_ssa_2020-__-7349
    ...                    ...              ...                           ...
    25178   input_file_w2_2020             7546   input_file_ssa_2020-__-2590
    25179   input_file_w2_2020             7547   input_file_ssa_2020-__-2590
    25180   input_file_w2_2020             8593  input_file_ssa_2020-__-10469
    25181   input_file_w2_2020             9215   input_file_ssa_2020-__-2971
    25182   input_file_w2_2020             9216   input_file_ssa_2020-__-2971
    [25183 rows x 3 columns]

As we can see, the pipeline has successfully outputted a ``Cluster ID`` for every 
input record it was able to link to another record for our probability threshold 
of ``.997``. ``Cluster ID`` names are chosen by Splink based on the first record 
assigned to them.

Running the pipeline also generates a :download:`DAG.svg <DAG-naive-pipeline.svg>` file in 
the results directory which shows the implementations, data dependencies and 
input validations present in the pipeline.

To see how the model linked records before assigning them to clusters, we can 
look at the intermediate output produced by the ``splink_evaluating_pairs`` 
implementation::

    $ pqprint results/2025_06_17_10_40_24/intermediate/splink_evaluating_pairs/result.parquet
            Left Record Dataset  Left Record ID Right Record Dataset  Right Record ID  Probability
    0       input_file_ssa_2020           16314   input_file_w2_2020             7604      0.00057
    1       input_file_ssa_2020           16318   input_file_w2_2020             7604      0.00057
    2       input_file_ssa_2020           16326   input_file_w2_2020             6049      0.00057
    3       input_file_ssa_2020           16351   input_file_w2_2020             3549      0.00057
    4       input_file_ssa_2020           16353   input_file_w2_2020             7434      0.00057
    ...                     ...             ...                  ...              ...          ...
    515790  input_file_ssa_2020           16309   input_file_w2_2020             7604      0.00057
    515791  input_file_ssa_2020           16310   input_file_w2_2020             7604      0.00057
    515792  input_file_ssa_2020           16311   input_file_w2_2020             7604      0.00057
    515793  input_file_ssa_2020           16312   input_file_w2_2020             7604      0.00057
    515794  input_file_ssa_2020           16313   input_file_w2_2020             7604      0.00057

    [515795 rows x 5 columns] 

The record pairs displayed in the preview are all far below the match threshold, but the results could 
be investigated further using ``pandas.read_parquet()`` in a Python session.

The Splink implementations in our pipeline also produce some diagnostic charts which can be useful 
for evaluating results, such as the :download:`match weights chart<naive_match_weights.html>` and 
:download:`comparison viewer tool<naive_comparison_viewer.html>`. These charts are available in the 
``diagnostics/splink_evaluating_pairs`` subdirectory of the results directory for each pipeline run.

Finally, since we generated the input datasets ourselves, and therefore know the ground truth of 
which records are truly links, lets see how our naive model performed. For a threshold match
probability of ``.997`` (chosen using match rate evaluation metrics), out of ``9262`` true links, 
we can calculate that our model results contained ``260`` false positives and ``43`` false negatives, 
with the rest of the links being accurate.


Configuring an improved pipeline
================================
Next, lets modify our naive pipeline configuration YAML to try to improve those results. Primarily, we 
will change the ``COMPARISONS`` we pass to ``splink_evaluating_pairs`` to use flexible comparison 
methods rather than exact matches, allowing us to link records which have typos or other noise in them. We'll 
use a new pipeline configuration YAML, :download:`pipeline_demo_improved.yaml`, with these changes.

In ``splink_evaluating_pairs``, our implementation configuration will now look like this::

    BLOCKING_RULES_FOR_TRAINING: "'l.first_name == r.first_name,l.last_name == r.last_name'"
    COMPARISONS: "'ssn:levenshtein,first_name:name,middle_initial:exact,last_name:name'"
    PROBABILITY_TWO_RANDOM_RECORDS_MATCH: 0.0000625  # == 1 / len(ssa)
    THRESHOLD_MATCH_PROBABILITY: 0
    LINK_ONLY: true

``COMPARISONS`` now uses 
`Levenshtein <https://moj-analytical-services.github.io/splink/api_docs/comparison_library.html#splink.comparison_library.LevenshteinAtThresholds>`_
comparisons for ``ssn``, and 
`Name <https://moj-analytical-services.github.io/splink/api_docs/comparison_library.html#splink.comparison_library.NameComparison>`_
comparisons for ``first_name`` and ``last_name``, to link similar but not identical SSNs and names.

We also use a more accurate value for 
`PROBABILITY_TWO_RANDOM_RECORDS_MATCH <https://moj-analytical-services.github.io/splink/api_docs/training.html#splink.internals.linker_components.training.LinkerTraining.estimate_parameters_using_expectation_maximisation>`_.

By re-running the pipeline with these changes, we can see how our results compare::

    $ easylink run -p pipeline_demo_improved.yaml -i input_data_demo.yaml -e environment_local.yaml -I /mnt/team/simulation_science/priv/engineering/er_ecosystem/images
    
For a threshold match
probability of ``.25`` (chosen using match rate evaluation metrics), out of ``9262`` true links, 
we can calculate that our model results contained ``236`` false positives and ``9`` false negatives, 
with the rest of the links being accurate.

The false negatives are significantly lower, thanks to our model linking more records with columns that 
are similar but don't exactly match. The false positives are only slightly lower, since ``216`` records, or
around 2%, are affected by 
`"borrow a social security number" <https://pseudopeople.readthedocs.io/en/latest/noise/column_noise.html#borrow-a-social-security-number>`_
pseudopeople noise. The ``SSA`` and ``W2`` dataset have extremely limited columns in common aside 
from SSNs (first, middle initial and last), which makes it difficult to link these "borrowed SSN" records.

Changing inputs
===============
Finally, lets run this same "improved" pipeline, but using :download:`input_data_demo_2030.yaml` 
as the input YAML, which uses the ``SSA`` and ``W2`` datasets from ``2030`` rather than 
``2020``::

    $ easylink run -p pipeline_demo_improved_2030.yaml -i input_data_demo_2030.yaml -e environment_local.yaml -I /mnt/team/simulation_science/priv/engineering/er_ecosystem/images
    
For a threshold match
probability of ``.25`` (chosen using match rate evaluation metrics), out of ``10345`` true links, 
we can calculate that our model results contained ``144`` false positives and ``7`` false negatives, 
with the rest of the links being accurate.
