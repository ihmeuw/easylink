steps:
  step_1:
    implementation:
      name: step_1_python_pandas
      # name: step_1_python_pyspark_distributed
      # TODO [MIC-4823 OR MIC-4824]: consider adding implementation-specific spark configurations here instead of in environment.yaml
      configuration:
        # DUMMY_CONTAINER_BROKEN: true
        DUMMY_CONTAINER_INCREMENT: 111
        # DUMMY_CONTAINER_MAIN_INPUT_FILE_PATHS:
        #   - "/input_data/original_input_data/input_file_1.parquet"
        # DUMMY_CONTAINER_SECONDARY_INPUT_FILE_PATHS: 
        #   - "/input_data/original_input_data/input_file_1.parquet"
  step_2:
    implementation:
      name: step_2_python_pandas
      # name: step_2_python_pyspark_distributed
      configuration:
        # DUMMY_CONTAINER_BROKEN: true
        DUMMY_CONTAINER_INCREMENT: 222
        # DUMMY_CONTAINER_MAIN_INPUT_FILE_PATHS:
        #   - "/input_data/original_input_data/input_file_1.parquet"
        # DUMMY_CONTAINER_SECONDARY_INPUT_FILE_PATHS: 
        #   - "/input_data/original_input_data/input_file_1.parquet"
